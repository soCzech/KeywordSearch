\chapter{Retrieval Model}

In this chapter, we describe retrieval model selection, employed dataset and model training. Later on we also discuss approaches to query formulation. In the next chapter we verify proposed methods by simulations.

\section{Model Selection}
As mentioned, there are multiple approaches towards textual annotation. For our task to create a model with at least a thousand classes we chose image classification. This is because object localization datasets are much smaller and resulting model would be more demanding on pre- and post-processing, negatively effecting already complicated processing pipeline.

Todays state-of-the-art approaches to the problem use exclusively deep neural networks, discussed in Related Work, thus we limit ourself to DCNNs. Every year there is a new architecture achieving better results, yet improved accuracy usually comes at cost of bigger models. Notable exception is NASNet network~\cite{zoph2017learning} that beats all other models in every size category; however, the model was released recently and hence it is not in our consideration. Many top performing models over the last few years were already presented thus we only show all considered models in Table \ref{fig:model_acc_vs_parameters}.

\begin{table}[h]
	
	\centering
	\sisetup{detect-weight=true,detect-inline-weight=math}
	\begin{tabular}{l@{\hspace{1cm}}cS[table-format=2.1]S[table-format=2.1]}
		\toprule
		\multirow{2}{*}{\textbf{Model}} & Number & \multicolumn{1}{c}{ILSVRC2012}  & \multicolumn{1}{c}{ILSVRC2012} \\
		& of parameters & \multicolumn{1}{c}{Top-1 Acc.\textsuperscript{$*$}} & \multicolumn{1}{c}{Top-5 Acc.\textsuperscript{$*$}}\\
		\midrule
		AlexNet \cite{AlexNet}  & 60 M & 59.3 \textsuperscript{$\dagger$} & 81.8 \textsuperscript{$\dagger$} \\
		GoogLeNet \cite{szegedy2015going}  & \textbf{6.6 M} & 69.8 \textsuperscript{$\ddagger$} & 89.9 \\
		VGG-16 \cite{simonyan2014very} & 138 M    & 74.4 & 91.9 \\
		Inception V3 \cite{szegedy2016rethinking}   & 23.8 M\textsuperscript{$\ddagger$}   & 78.8 & 94.4 \\
		Inception-ResNet-v2 \cite{szegedy2017inception} & 55.8 M\textsuperscript{$\ddagger$} & \bftabnum 80.1 & \bftabnum 95.1 \\
		\bottomrule
		\multicolumn{4}{l}{\footnotesize \textsuperscript{$*$} Without averaging over multiple cropped images and ensemble of models.} \\
		\multicolumn{4}{l}{\footnotesize \textsuperscript{$\dagger$} Achieved by averaging four corner patches and a center patch of image.} \\
		\multicolumn{4}{l}{\footnotesize \textsuperscript{$\ddagger$} Value taken from \cite{zoph2017learning}.}
	\end{tabular}
	
	\caption{Considered neural networks and their performance.}
	\label{fig:model_acc_vs_parameters}
\end{table}


With limited computational resources, emphasis in our decision was given on model size. We selected GoogLeNet since its size allowed us to train it on GPU with 2 GB of memory even with batches of size 64 reasonably fast. Our decision is also backed by \cite{ModelSizes2016} where the network ranked among the most efficient ones utilizing well its parameter space. The network consists of initial convolution layers with max pooling reducing spatial dimensions of input to $28\times 28\times 192$ followed by 9 Inception blocks (shown in Figure \ref{fig:inception_block}) interleaved by occasional max pooling with output dimensions $7\times 7\times 1024$. Then there is an average pooling further reducing output to $1\times 1\times 1024$. The last layers are depicted in Figure \ref{fig:inception_end} also with modifications discussed in Chapter \ref{}. For the ones more interested in the architecture, we point to the original paper \cite{szegedy2015going} for detailed description of the network. 



\begin{figure}
	\centering
	
	\begin{tabular}{@{}c@{}}
		\subfloat{
			\input{img/inception_ori_end.tikz}
		}
	\end{tabular}
	\begin{tabular}{@{}c@{}}
		\subfloat{
			\input{img/inception_new_end.tikz}
		}
	\end{tabular}
	
	\caption[Last layers of GoogLeNet]{Last layers of GoogLeNet with their output sizes as presented in \cite{szegedy2015going} (left) and modified version used in our experiments with changes in red (right).}
	\label{fig:inception_end}
\end{figure}



\section{Dataset Selection}
Due to the unmatched number of classes to chose from and extensive use in scientific community, we have decided to use ImageNet database \cite{ILSVRC15} as a starting point. The database is based on WordNet \cite{WordNet}, giving us an advantage when designing search model later in Chapter \ref{}. The most used dataset ILSVRC2012 has a few flaws however. It was designed for a competition thus it does not contain many common objects, yet on the other hand its 120 dog classes could pose challenge when searching because even a human could then make a mistake classifying a searched dog incorrectly. Bigger ImageNet10K \cite{deng2010does}, despite solving the lack of common objects, amplifies possible human error and greatly reduces performance of any neural network. We thus decided to create our own dataset. All classes with at least 1000 example images from Winter 2011 release of ImageNet were taken into consideration, yielding to 6642 classes in total.

We tried selecting classes for our network automatically from WordNet structure, yet results were unsatisfactory therefore human judge was used to select viable classes. To simplify selection and orientation in the classes, we propose following steps:
\begin{enumerate}
	\item Mapping $\mathcal{M}$ of each image class to $n$-dimensional vector space is computed.
	\item Image classes are divided into $k$ clusters (we used $k=50$).
	\item For each cluster, WordNet tree is created enabling closer inspection of similar classes.
\end{enumerate}
We defined mapping $\mathcal{M}$ of image class $c$ as
\begin{equation}
\mathcal{M}(c)=\mathbb{E}_{\bm{x}\in c}f(\bm{x};\bm{\theta})\approx\frac{1}{\abs{S_c}}\sum_{\bm{x}\in S_c}f(\bm{x};\bm{\theta})
\end{equation}
where $\bm{x}$ are images representing class $c$, $f(\cdot;\bm{\theta})$ is a function parametrized by $\bm{\theta}$ mapping image to $n$-dimensional vector and $S_c$ is subset of images of class $c$ taken from ImageNet database. Since DCNNs proved to be efficient in mapping images to lower dimensional space~\cite{donahue2014decaf}, standard GoogLeNet is used as a good $f$. The network is initialized to publicly available weights \cite{TFmodels} trained on ILSVRC2012 with the last linear layer removed. In our experiments $\abs{S_c}=10$ proved to be sufficient approximation of mean value.

The WordNet structure, no matter how precise in words' meaning, does not reflect human visual perceptions. For example a man can quite easily associate a picture to word \textit{barbecue}, yet it can have two meanings -- \textit{a cooker} and \textit{a meal} that are far apart in WordNet. Unfortunately the images associated with those categories are similar, therefore confusing a neural network and adding ambiguity to user's search. In order for a human judge to identify such conflicts, grouping visually similar images is necessary, since one cannot orientate in thousands of classes. As demonstrated in \cite{dasgupta2008hardnessKmeans}, clustering even into two clusters is NP-hard. We thus use open-source implementation \cite{scikit-learn} of $k$-means algorithm with careful seeding known as $k$-means++ \cite{kmeans} where cluster centers are chosen with probability proportional to square of their distance to the closest already chosen center. With this approach, multiple clusters contained only classes of one kind, such as \textit{a person}, \textit{a tree}, \textit{a bird} or \textit{a horse}, the last one spread over the whole WordNet structure in synsets connected to \textit{racing}, \textit{hunting}, \textit{transportation}, \textit{people}, \textit{buildings} and even \textit{clothing} hence demonstrating its usefulness. On the other hand the WordNet tree also proved valuable revealing multiple synsets with wrongly assigned images based on ambiguity in their names. For example, two synsets \textit{a smoker} and \textit{a nonsmoker} with descriptions \textit{`A passenger [railway] car for passengers who wish to (or not to) smoke'} contain photos of people smoking and photos of automobiles respectively.


\begin{figure}
	\centering
	\scalebox{0.8}{\input{img/dataset_clusters.pgf}}
	\caption[Projection of ImageNet classes into 2D]{6642 ImageNet classes in 50 clusters projected into 2D plane. Higher dimensional vectors $\mathcal{M}(c)$ are clustered using $k$-means++ \cite{kmeans} algorithm. Each vector is then projected into 2D using principal component analysis \cite{pca}. For example, the rightmost clusters contain only flowers and trees.}
	\label{fig:dataset_clusters}
\end{figure}

\subsection{Image Preprocessing}
Using the outlined method, 1150 image categories were chosen out of the 6642 considered (see Appendix \ref{} for complete list) and corresponding images were downloaded from \cite{ImageNetDownload}. Only 1000 images per each class were selected to reduce size of the dataset and to equalize number of images in each class. Every image was then resized in a way that its shorter size had 400 pixels further reducing the dataset size but still enabling cropping as a dataset augmentation method during training. Furthermore, train and validation datasets were created. The train dataset containing nine-tenths of images of each class and the validation dataset containing the rest. Creation of a test set was unnecessary since we have not been designing completely new network architecture.

During training we employ several data augmentation methods such as \cite{simard2003best} to reduce overfitting. Firstly, each image is randomly cropped. We ensure the new image has at least 80\% of area of the original image. Then brightness and saturation are randomly adjusted and each image is with probability 1/2 vertically flipped. All the methods are not computationally intensive and therefore they can be performed online during training without the need to store distorted images on a disk.


\section{Model Training}
Our goal is to train a model parametrized by $\bm{\theta}$ to map set of images $\bm{X}$ to set of labels $Y$. Assuming $(\bm{x},y)\in(\bm{X},Y)$ are drawn from true but unknown distribution $p_{world}$ independently, the objective can be written as maximum likelihood estimation in following way:
\begin{align}
\bm{\theta}_{ML} &=\mathop{\arg\max}_{\bm{\theta}}\prod\limits_{i=1}^{n} p_{model}\left(y^{(i)}|\bm{x}^{(i)};\bm{\theta}\right) \\
&=\mathop{\arg\max}_{\bm{\theta}}\sum\limits_{i=1}^{n} \log p_{model}\left(y^{(i)}|\bm{x}^{(i)};\bm{\theta}\right)
\end{align}
The second equality holds because logarithm is always increasing function and $\log(ab)=\log(a)+\log(b)$. Further dividing by $n$, using empirical distribution $\hat{p}_{data}$ and minimizing negative of the original function we can write:
\begin{equation}
\mathop{\arg\min}_{\bm{\theta}}-\mathbb{E}_{(\bm{x},y)\sim \hat{p}_{data}}\left[\log p_{model}\left(y|\bm{x};\bm{\theta}\right)\right]
\end{equation}
Such function can be optimized using stochastic gradient descent (SGD) as described in Chapter \ref{}. However for training the network we use more advanced algorithm based on SGD called Adam~\cite{adam} which computes individual adaptive learning rates for
different parameters from estimates of first and second moments of the gradients. Default values of $\beta_1 = 0.9$ and $\beta_2 = 0.999$ were used.

Training as big models as GoogLeNet~\cite{szegedy2015going} is requires a lot of computing power that could potentially be wasteful since GoogLeNet has been already trained on images with similar low level features. Therefore we use transfer learning technique firstly described by~\cite{donahue2014decaf}. This approach cuts training time needed by initializing model's weights to values of a model trained on different but related task (normally, prior to a training, model weights would be initialized to random values such as~\cite{glorot2010understanding}). In our work, we use TensorFlow~\cite{tensorflow2015}, an open-source machine learning framework with extensive collection of pretrained models~\cite{TFmodels} also containing GoogLeNet with weights trained on ILSVRC2012 dataset. We use those initial weights for the whole network except for the last layer which is replaced by a new one of different dimension (see Figure \ref{fig:inception_end}) with its weights initialized from a uniform distribution $U\left[-\sqrt{6/(n_j+n_{j+1})}, \sqrt{6/(n_j+n_{j+1})}\right]$ where $n_j+n_{j+1}$ is number of neurons in two adjacent layers (see~\cite{glorot2010understanding} for more details).

During initial phrase of training (Figure \ref{fig:train_progress} in red) only the last layer was trained with the other weights were fixed, because in the beginning random gradients can destroy learned low-level features worth keeping. In the initial phrase the network was trained by 15,000 batches of size 64, firstly with learning rate $10^{-4}$ and after 7,500 batches with $5\cdot 10^{-5}$.


\begin{figure}
	\centering
	\input{img/nn_train_progress.tikz}
	
	\caption[Accuracy of GoogLeNet during training]{Accuracy of GoogLeNet~\cite{szegedy2015going} during training on random part of validation dataset. Horizontal axis shows number of images the model has seen to a given point in training. Initial training of the last layer only is shown in red. Curves are smoothed by exponential moving average, i.e. $S_i=\frac{1}{2}V_i + \frac{1}{2}S_{i-1}$ where $V_i$ is model's accuracy and $S_i$ is shown smoothed value.}
	\label{fig:train_progress}
\end{figure}




\begin{table}[h]
	
	\centering
	\sisetup{detect-weight=true,detect-inline-weight=math}
	\begin{tabular}{l@{\hspace{1cm}}cS[table-format=2.1]S[table-format=2.1]}
		\toprule
		Evaluation Method & \multicolumn{1}{c}{Top-1 Acc.} & \multicolumn{1}{c}{Top-5 Acc.} & \multicolumn{1}{c}{Top-10 Acc.}\\
		\midrule
		center cutout & 56.1 & 83.8 & 90.2 \\
		10 patches averaged & 57.1 & 84.7 & 90.9 \\
		\bottomrule
	\end{tabular}
	
	\caption{Trained model performance on the validation set.}
	\label{fig:trained_model_acc}
\end{table}
