\chapter{Retrieval Model}\label{chap:retrieval_model}
Modern KIS tools suitable for large or very large collections depend on automatic retrieval models. We select the appropriate model for our task and propose modifications needed for the model to be useful for KIS. We also describe steps how we created training data for our model.

\section{Model Selection}
As mentioned, there are multiple approaches towards textual annotation. For our task to create a model with at least a thousand classes we chose image classification. This is because object localization datasets are much smaller and a resulting model would be more demanding on pre- and post-processing, negatively affecting already complicated processing pipeline.

Today's state-of-the-art approaches to the problem use exclusively deep neural networks, discussed in Related Work, thus we limit ourself to DCNNs. Every year there is a new architecture achieving better results, yet improved accuracy usually comes at cost of bigger models. A notable exception is NASNet network~\cite{zoph2017learning} that beats all other models in every size category; however, the model was released recently and hence it is not in our consideration. Many top performing models over the last few years were already presented thus we only show all considered models in Table \ref{fig:model_acc_vs_parameters}.

\begin{table}[h]
	
	\centering
	\sisetup{detect-weight=true,detect-inline-weight=math}
	\begin{tabular}{l@{\hspace{1cm}}cS[table-format=2.1]S[table-format=2.1]}
		\toprule
		\multirow{2}{*}{\textbf{Model}} & Number & \multicolumn{1}{c}{ILSVRC2012}  & \multicolumn{1}{c}{ILSVRC2012} \\
		& of parameters & \multicolumn{1}{c}{Top-1 Acc.\textsuperscript{$*$}} & \multicolumn{1}{c}{Top-5 Acc.\textsuperscript{$*$}}\\
		\midrule
		AlexNet \cite{AlexNet}  & 60 M & 59.3 \textsuperscript{$\dagger$} & 81.8 \textsuperscript{$\dagger$} \\
		GoogLeNet \cite{szegedy2015going}  & \textbf{6.6 M} & 69.8 \textsuperscript{$\ddagger$} & 89.9 \\
		VGG-16 \cite{simonyan2014very} & 138 M    & 74.4 & 91.9 \\
		Inception V3 \cite{szegedy2016rethinking}   & 23.8 M\textsuperscript{$\ddagger$}   & 78.8 & 94.4 \\
		Inception-ResNet-v2 \cite{szegedy2017inception} & 55.8 M\textsuperscript{$\ddagger$} & \bftabnum 80.1 & \bftabnum 95.1 \\
		\bottomrule
		\multicolumn{4}{l}{\footnotesize \textsuperscript{$*$} Without averaging over multiple cropped images and ensemble of models.} \\
		\multicolumn{4}{l}{\footnotesize \textsuperscript{$\dagger$} Achieved by averaging four corner patches and a center patch of image.} \\
		\multicolumn{4}{l}{\footnotesize \textsuperscript{$\ddagger$} Value taken from \cite{zoph2017learning}.}
	\end{tabular}
	
	\caption[Considered neural networks and their performance]{Considered neural networks and their performance.}
	\label{fig:model_acc_vs_parameters}
\end{table}


With limited computational resources, emphasis in our decision was given on model size. We selected GoogLeNet since its size allowed us to train it on GPU with 2 GB of memory with batches of size 32 reasonably fast. Our decision is also supported by \cite{ModelSizes2016} where the network ranked among the most efficient ones utilizing well its parameter space. The network consists of initial convolution layers with max pooling reducing spatial dimensions of input to $28\times 28\times 192$ followed by 9 Inception blocks (shown in Figure \ref{fig:inception_block}) interleaved by occasional max pooling and with output dimensions $7\times 7\times 1024$. Then there is an average pooling further reducing output to $1\times 1\times 1024$. The last layers are depicted in Figure \ref{fig:inception_end} also with modifications discussed in the next sections. For the ones more interested in the architecture, we point to the original paper \cite{szegedy2015going} for the detailed description of the network and \cite{ioffe2015batch} for modifications by adding batch normalization to layer outputs.



\begin{figure}
	\centering
	
	\begin{tabular}{@{}c@{}}
		\subfloat{
			\input{img/inception_ori_end.tikz}
		}
	\end{tabular}
	\begin{tabular}{@{}c@{}}
		\subfloat{
			\input{img/inception_new_end.tikz}
		}
	\end{tabular}
	
	\caption[The last layers of GoogLeNet]{The last layers of GoogLeNet with their output sizes as presented in \cite{szegedy2015going} (left) and modified version used in our experiments with changes in red (right).}
	\label{fig:inception_end}
\end{figure}



\section{Dataset Selection}\label{sec:dataset_selection}
Due to the unmatched number of classes to chose from and extensive use in the scientific community, we have decided to use the ImageNet database \cite{ILSVRC15} as a starting point. The database is based on WordNet \cite{WordNet}, giving us an advantage when designing search model later in Chapter \ref{chap:text_search}. The most used dataset ILSVRC2012 has a few flaws, however. It was designed for a competition thus it does not contain many common objects, yet on the other hand, its 120 dog classes could pose a challenge when searching because even a human could then make a mistake classifying a searched dog incorrectly. Bigger ImageNet10K \cite{deng2010does}, despite solving the lack of common objects, amplifies possible human error and greatly reduces the performance of any neural network. We thus decided to create our own dataset. All classes with at least 1000 example images from Winter 2011 release of ImageNet were taken into consideration, yielding to 6642 classes in total.

We tried selecting classes for our network automatically from WordNet structure, yet results were unsatisfactory therefore human judge was used to select viable classes. To simplify selection and orientation in the classes, we propose the following steps:
\begin{enumerate}
	\item Mapping $\mathcal{M}$ of each image class to $n$-dimensional vector space is computed.
	\item Image classes are divided into $k$ clusters (we used $k=50$).
	\item For each cluster, WordNet directed acyclic graph (DAG) with edges representing hypernym--hyponym relation is created enabling closer inspection of similar classes.
\end{enumerate}
We defined mapping $\mathcal{M}$ of image class $c$ as
\begin{equation}
\mathcal{M}(c)=\mathbb{E}_{\bm{x}\sim \hat{p}_{c}}f(\bm{x};\bm{\theta})\approx\frac{1}{\abs{S_c}}\sum_{\bm{x}\in S_c}f(\bm{x};\bm{\theta})
\end{equation}
where $\bm{x}$ are images drawn from distribution $\hat{p}_{c}$ representing class $c$, $f(\cdot;\bm{\theta})$ is a function parametrized by $\bm{\theta}$ mapping image to $n$-dimensional vector and $S_c$ is a subset of images of class $c$ taken from ImageNet database. Since DCNNs proved to be efficient in mapping images to lower dimensional space~\cite{donahue2014decaf}, standard GoogLeNet is used as a good $f$. The network is initialized to publicly available weights \cite{TFmodels} trained on ILSVRC2012 with the last linear layer with softmax classifier removed. In our experiments $\abs{S_c}=10$ proved to be sufficient approximation of the mean value.

WordNet structure, no matter how precise in words' meaning, does not reflect human visual perceptions. For example, a man can quite easily associate a picture to word \textit{barbecue}, yet it can have two meanings -- \textit{a cooker} and \textit{a meal} that are far apart in WordNet. Unfortunately, the images associated with those categories are similar, therefore confusing a neural network and adding ambiguity to a user's search. In order for a human judge to identify such conflicts, grouping visually similar images is necessary, since one cannot orientate in thousands of classes. We use an open-source implementation \cite{scikit-learn} of $k$-means clustering algorithm to group image classes into clusters. Even though clustering into two clusters is already NP-hard~\cite{dasgupta2008hardnessKmeans}, using a version of the algorithm known as $k$-means++~\cite{kmeans}, where cluster centers are chosen with probability proportional to the square of their distance to the closest already chosen center, resulted into having multiple clusters with only one kind of an object, such as \textit{a~person}, \textit{a~tree}, \textit{a~bird} or \textit{a~horse}. For example, the \textit{horse} cluster contained synsets from multiple branches of WordNet structure connected to \textit{racing}, \textit{hunting}, \textit{transportation}, \textit{people}, \textit{buildings} and even \textit{clothing} hence demonstrating the usefulness of the clustering. Further, we constructed WordNet DAG for each cluster and selected useful synsets. Such an approach also revealed multiple synsets with wrongly assigned images based on ambiguity in their names. For example, two synsets \textit{a smoker} and \textit{a nonsmoker} with descriptions \textit{`A passenger [railway] car for passengers who wish to (or not to) smoke'} contain photos of people smoking and photos of automobiles respectively.


\begin{figure}
	\centering
	\scalebox{0.8}{\input{img/dataset_clusters.pgf}}
	\caption[Projection of ImageNet classes into 2D]{6642 ImageNet classes in 50 clusters projected into a 2D plane. Higher dimensional vectors $\mathcal{M}(c)$ are clustered using $k$-means++ \cite{kmeans} algorithm. Each vector is then projected into 2D using principal component analysis \cite{pca}. For example, the rightmost clusters in the image contain only flowers and trees.}
	\label{fig:dataset_clusters}
\end{figure}

\subsection{Image Preprocessing}
Using the outlined method, 1150 image categories were chosen out of the 6642 considered (see Attachment~\ref{att:classes} for the complete list) and corresponding images were downloaded from \cite{ImageNetDownload}. Only 1000 images per each class were selected to reduce the size of the dataset and to equalize the number of images in each class. Every image was then resized in a way that its shorter size had 400 pixels further reducing the dataset size but still enabling cropping as a dataset augmentation method during training. Furthermore, train and validation datasets were created. The train dataset containing nine-tenths of images of each class and the validation dataset containing the rest. Creation of a test set was unnecessary since we have not been designing completely new network architecture.

During training, we employ several data augmentation methods such as \cite{simard2003best} to reduce overfitting. Firstly, each image is randomly cropped. We ensure the new image has at least 80\% of the area of the original image. Then the brightness and saturation are randomly adjusted and each image is with probability 1/2 vertically flipped. All the methods are not computationally intensive and therefore they can be performed online during training without the need to store distorted images on a disk.


\section{Model Training}
Our goal is to train a model parametrized by $\bm{\theta}$ to map a set of images $\bm{X}$ to a set of labels $Y$. Assuming $(\bm{x},y)\in(\bm{X},Y)$ are drawn from true but unknown distribution $p_{world}$ independently, the objective can be written as maximum likelihood estimation in following way (see \cite{Goodfellow-et-al-2016} for more details):
\begin{align}
\bm{\theta}_{ML} &=\mathop{\arg\max}_{\bm{\theta}}\prod\limits_{i=1}^{n} p_{model}\left(y^{(i)}\given\bm{x}^{(i)};\bm{\theta}\right) \\
&=\mathop{\arg\max}_{\bm{\theta}}\sum\limits_{i=1}^{n} \log p_{model}\left(y^{(i)}\given\bm{x}^{(i)};\bm{\theta}\right)
\end{align}
The second equality holds because logarithm is always increasing function and $\log(ab)=\log(a)+\log(b)$. Further dividing by $n$, using empirical distribution $\hat{p}_{data}$ and minimizing negative of the original function we can write:
\begin{equation}
\mathop{\arg\min}_{\bm{\theta}}-\mathbb{E}_{(\bm{x},y)\sim \hat{p}_{data}}\left[\log p_{model}\left(y\given\bm{x};\bm{\theta}\right)\right]\label{eq:mle}
\end{equation}
Such function can be optimized using stochastic gradient descent (SGD) as described in Chapter \ref{chap:preliminaries}. However, for training the network, we use a more advanced algorithm based on SGD called Adam~\cite{adam} which computes individual adaptive learning rates for
different parameters from estimates of first and second moments of the gradients. Default values of $\beta_1 = 0.9$ and $\beta_2 = 0.999$ were used.

Training as big models as GoogLeNet~\cite{szegedy2015going} requires a lot of computing power that could be potentially wasteful since GoogLeNet has been already trained on images with similar low-level features. Therefore we use a transfer learning technique firstly described by~\cite{donahue2014decaf}. This approach cuts training time needed by initializing model's weights to values of a model trained on different but related task (normally, prior to a training, model weights would be initialized to random values such as~\cite{glorot2010understanding}). In our work, we use TensorFlow~\cite{tensorflow2015}, an open-source machine learning framework with an extensive collection of pretrained models~\cite{TFmodels} also containing GoogLeNet with weights trained on the ILSVRC2012 dataset. We use those initial weights for the whole network except for the last layer which is replaced by a new one of different dimension (see Figure \ref{fig:inception_end}) with its weights initialized from a uniform distribution $U\left[-\sqrt{6/(n_j+n_{j+1})}, \sqrt{6/(n_j+n_{j+1})}\right]$ where $n_j+n_{j+1}$ is number of neurons in two adjacent layers (see~\cite{glorot2010understanding} for more details).

During the initial phase of training (Figure \ref{fig:train_progress} in red) only the last layer was trained with the other weights fixed because in the beginning random gradients can destroy learned low-level features worth keeping. The first phase consisted of 15,000 batches of size 64, firstly with learning rate $10^{-4}$ and after 7,500 batches with $5\cdot 10^{-5}$. Further in the training, the whole network was fine-tuned with the batch size of 32 examples and learning rate between $5\cdot 10^{-5}$ and $10^{-6}$ as shown in Figure~\ref{fig:train_progress}. The selected batch size is due to limited GPU memory and it is possible that better results could be obtained if a bigger batch size was used, even though the paper discussing batch normalization of GoogLeNet~\cite{ioffe2015batch} also uses the batch size~of~32.


\begin{figure}
	\centering
	\input{img/nn_train_progress.tikz}
	
	\caption[Accuracy of GoogLeNet during training]{The accuracy of GoogLeNet~\cite{szegedy2015going} during training on a random subset of the validation dataset. The horizontal axis shows the number of images the model has seen to a given point in training. Initial training of the last layer only is shown in red. Curves are smoothed by exponential moving average, i.e. $S_i=\frac{1}{2}V_i + \frac{1}{2}S_{i-1}$ where $V_i$ is model's accuracy and $S_i$ is the shown smoothed value.}
	\label{fig:train_progress}
\end{figure}

\begin{table}[h]
	
	\centering
	\sisetup{detect-weight=true,detect-inline-weight=math}
	\begin{tabular}{l@{\hspace{1cm}}S[table-format=2.1]S[table-format=2.1]S[table-format=2.1]}
		\toprule
		Evaluation Method & \multicolumn{1}{c}{Top-1 Acc.} & \multicolumn{1}{c}{Top-5 Acc.} & \multicolumn{1}{c}{Top-10 Acc.}\\
		\midrule
		baseline (whole image) & 56.0 & 83.8 & 90.1 \\
		center cutout & 56.1 & 83.8 & 90.2 \\
		10 patches averaged & \bftabnum 57.1 & \bftabnum 84.7 & \bftabnum 90.9 \\
		\bottomrule
	\end{tabular}
	
	\caption[Trained model performance on the validation set]{Trained model performance on the validation set.}
	\label{fig:trained_model_acc}
\end{table}

At the test time, three evaluation methods were tried. As a baseline, images were resized using bilinear interpolation to the network's input size ($224\times 224$). The second approach was to take a center patch of an image with width and height of $87,5\%$ of the original image again resized by bilinear interpolation to the network's input size. The third approach used a similar technique as described in~\cite{AlexNet}. Additionally to the center patch, it takes four $224\times 224$ corner patches from a rescaled image of size $320\times 320$ and averages the predictions made by the networkâ€™s softmax layer on them as well as on their horizontal reflections. Results of all methods are summarized in Table \ref{fig:trained_model_acc}.

