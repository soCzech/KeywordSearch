\chapter{Textual Search}\label{chap:text_search}

The model described in the previous chapter assigns each image $\bm{x}$ a vector $\bm{\hat{y}}$. In theory, it represents how likely each class is contained in the image. However, this representation is far from useful for a user. Therefore multiple techniques are used to ensure the model is a convenient yet powerful search tool even for novices.

\section{Supported Labels}
Our tool supports only a finite set of labels $L$ that are prompted by the interface when users form the query described in greater detail in Section \ref{chap:application_prototype}. It could be possible to construct a query given higher level description transformed by a task-specific set of neural language processing rules \cite{moumtzidou2017verge}, but this approach may mislead user if a searched high-level concept is not in an internal model. Thus a set $L_m$ is created containing labels corresponding to the image classes with their names, descriptions and hyponyms taken from WordNet \cite{WordNet}. We further utilize WordNet structure creating a new set $L_h$ of hypernyms of all labels in $L_m$. The final set of labels is then $L=L_m\cup L_h$. With hypernym--hyponym relation as a directed edge, $L$ can be viewed as a directed acyclic graph (DAG) where all vertices (i.e. labels) with no outgoing edges are in $L_m$ (however, there can be vertices in $L_m$ with outgoing edges).

\section{Query Formulation and Ranking}\label{sec:query_formulation_and_ranking}
The users are allowed to specify sets of supported labels $N_i \subseteq L$, wherein each set the labels are connected by logical \textsf{OR}, while the sets $N_i$ are connected by logical \textsf{AND}. For $k$ such sets, the user query $Q_u$ is then written as
\begin{equation}
	Q_u=\bigwedge\limits_{i=1}^k\left(\bigvee\limits_{\forall label_j\in N_i} label_j \right)\label{eq:cnf}
\end{equation}
and further in the text, for convenience, we use data representation of $Q_u$ defined as $\{N_i\}_{i=1}^k$ with the same meaning as in Equation \ref{eq:cnf}. If the query contains any hypernyms (labels with some outgoing edges in $L$), further preprocessing needs to be done. By default, every hypernym $h$ is substituted by a set of all labels in $L_m$ that are reachable from $h$ in DAG $L$. For hypernyms in $L_m$ (i.e. they correspond to a class recognized by the model), the substitution can be disabled. Finally, the preprocessed query $Q_p$ contains only labels from $L_m$ directly recognized by our model.

The ranking $r(\cdot)$ for each image $\bm{x}$, given preprocessed query $Q_p$ and model parametrized by $\bm{\theta}$, is calculated as 
\begin{equation}
r\left(\bm{x}; Q_p, \bm{\theta}\right)=\prod\limits_{\forall N_i \in Q_p}\left(
	\sum\limits_{\forall label_j\in N_i} \bm{\hat{y}}_{label_j}\cdot idf\left(label_j\right)
\right)\label{eq:text_rank}
\end{equation}
where $\bm{\hat{y}}=f\left(\bm{x}; \bm{\theta}\right)$  is model prediction on the image $\bm{x}$ and $\bm{\hat{y}}_{label_j}$ represents its relevance score of containing $label_j$ and $idf(\cdot)$ represents an inverse document frequency (IDF)~\cite{baeza1999modern}. Let us explain the underlying reasoning why to introduce IDF on an example: When a user searches \textit{a person in front of an excavator}, person keyword completely dominates the query since in the dataset there are thousand times as many persons as excavators. Standard IDF, however, uses the number of documents over the number of documents containing a given term which is unusable because softmax layer gives small nonzero values to almost all classes. We thus define inverse document frequency as
\begin{equation}
idf(label) = \log\left(
\frac{
\max\limits_{i\in labels} \sum_{\bm{x}}\hat{\bm{y}}_{i}
}{
\sum_{\bm{x}}\hat{\bm{y}}_{label}
} + 1\right)\label{eq:idf}
\end{equation}
where $\hat{\bm{y}}$ is the same as in Equation \ref{eq:text_rank}, therefore dependent on image $\bm{x}$. If we think of $\hat{\bm{y}}_i$ as a probability image $\bm{x}$ contains label $i$, the fraction in Equation \ref{eq:idf} can be viewed as how likely any given image contains the most common label in a collection over how likely it contains the searched label.

\section{Application Prototype}\label{chap:application_prototype}
This section discusses our user-friendly implementation of described approaches. Due to the project we are taking part in, the user interface is programmed in C\# language using the .NET framework and only focuses on an implementation of keyword search since the rest is developed jointly. Also note that model training, simulations, data annotation and other offline tasks presented in this work are programmed in Python and are available together with documentation on the attached CD (Attachment \ref{att:cd}). The user application can be logically divided into three parts described in the following sections: \textit{Main Window}, \textit{Keyword Model} and \textit{Suggestion Text Box}. Overview of the most important classes, their methods and interactions can be seen in Figure~\ref{fig:wpf_app}.

\begin{figure}[ht]
	\centering
	\input{img/wpf_app_uml.tikz}
	
	\caption[Application prototype structure]{Application prototype structure. A solid arrow represents a reference to a class instance. Multiple arrows denote, that reference to more instances is possible. A dotted arrow represents a reference to a function. Interaction with external data files is denoted by a database icon.}
	\label{fig:wpf_app}
\end{figure}

\subsection{Main Window}
The Main Window class is used as an entry point for the application defining its user interface and connecting the text box to the retrieval model. It is also responsible for correctly initializing all the components with appropriate external data files defined in a configuration file. Further, since the application is only a demo, the class loads and displays images to a user based on a model's ranking.


\subsection{Keyword Model}
Keyword model implements the ranking algorithm as described in the section above. An inverted index is utilized for fast query retrieval. As the creation of the index is the most demanding operation due to the image annotation by DCNN, it is created once in the preprocessing phase. The neural network assigns each image $\bm{x}^{(i)}$ set of labels with its score $\{(label_j,\hat{\bm{y}}_{label_j})\given{\hat{\bm{y}}=f(\bm{x}^{(i)};\bm{\theta})},\  {label_j\in L_m}\}$. Further, labels with a score lower than $0.001$ are discarded without loss of model's quality as discussed in Chapter~\ref{chap:evaluation}. The inverted index then contains for each label $j\in L_m$ list of tuples $(i, \hat{\bm{y}}_{label_j})$ -- image id and assigned score. The file also incorporates label-to-offset map that enables fast seek instructions to individual label lists.

During application startup, only the label-to-offset map is loaded into RAM. Lists corresponding to the given labels are copied into the memory only when the model is queried by a user. For small datasets, the overhead is negligible since there are more I/O intensive operations such as image rendering, etc. For large collections, the slowdown can be noticed if a compound query is created yet this approach saves space in RAM that can be used by other more memory demanding models such as similarity model. Also note, that the most labels are not used each time the application is run since only a handful of keywords are used per each target image. To mitigate the possible slowdown the model uses two caches. The bigger one stores already loaded labels avoiding virtually any repetitive label read during regular usage. The smaller one stores clauses (i.e. $\bigvee_i label_i$) enabling faster retrieval of repeated or more complex queries. The application also supports multiple keyword sources, therefore, Keyword Model Wrapper is used to redirect model calls to appropriate models.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.5]{img/keyword-textbox.png}
	
	\caption[Keyword search interface]{Keyword search interface. When selecting the \textit{flower} label, hyponyms \textit{pink}, \textit{poppy}, etc. will also be searched. To select only \textit{flower}, a user can hold the control key while pressing the enter. If the label is expanded by pressing the right key, the user can also select only individual flowers.}
	\label{fig:keyword-textbox}
\end{figure}

\subsection{Suggestion Text Box}
The suggestion-providing text box is comprised of multiple classes encapsulated by Keyword Search Control class enabling effortless integration to any existing application. If text box's content changes, a handler in Keyword Search Control selects Suggestion Provider belonging to the currently selected keyword source. The provider asynchronously searches all labels $l\in L$ and hands over the results to the text box when completed. Suggestion Popup is then populated with the results, displaying them to a user. If any suggestion is selected to update the query, Keyword Search Control passes a list of selected labels to a registered handler responsible for a query ranking.

The text box accepts only the labels $L$, however, full-text search over their WordNet names and descriptions is performed using Aho--Corasick string search algorithm. This way, a user gets suggestions even if the desired label is not present and thus can select any similar label based on its description or quickly see that the query needs to be formulated differently. As user types, the suggestions are presented with the searched phrase highlighted (see Figure \ref{fig:keyword-textbox}). The labels are distinguished by color. Red labels are the added WordNet hypernyms $L_h$, green and gray labels represent basic 1150 labels $L_m$. The green labels represent hypernyms for gray labels. The suggestions are browsed by up and down keys, while the selection of keyword is done only by enter. We find mouse control unnecessary since interaction via keyboard is faster, but we may include more intuitional and user-friendly mouse control for novices as well. When a user browses to a hypernym, the right key can be used to expand the hypernym, following the WordNet structure. When a label is selected, it appears on the top panel, connected by logical \textsf{OR} with previously selected labels. Logical relation between the labels can be changed to \textsf{AND} and back by left mouse click, clicking on the labels themselves removes them from the query.

\section{Other Retrieval Models}
Our work is part of a bigger project focused on video retrieval with a goal of creating a tool that provides a user with a powerful means to interactively search his video collection. In this section, we therefore summarize our tool's other retrieval models and model fusion used for Lifelog Search Challenge (LSC) workshop at ICMR2018~\cite{LokocLSC} and for Video Browser Showdown (VBS) competition at MMM2018~\cite{lokovc2018revisiting}.

Mainly for visual KIS tasks, our tool features easy-to-use canvas for color sketches. Every sketch is represented as a set $Q_c = \{\left(q_i, r_i, t_i\right)\}^k_{i=1}$ of $k$ color points where $q_i\in\R^3$ is color in CIE Lab color space, $r_i$ represents region where the given color shall be located and $t_i\in \{\mathrm{ALL},\mathrm{ANY}\}$ specifies whether ALL or just ANY of the pixels in the region should be considered. The ranking $c(\cdot)$ of an image $\bm{x}$ is then computed as
\begin{equation}
c\left(\bm{x}; Q_c\right) =-\left(\sum\limits_{\substack{\forall \left(q_i,r_i,t_i\right) \in Q_c\\t_i=\mathrm{ANY}}} \min\limits_{p\in\bm{x}\,\mathrm{in}\,r_i}L_2\left(q_i, p\right)+\sum\limits_{\substack{\forall \left(q_i,r_i,t_i\right) \in Q_c\\t_i=\mathrm{ALL}}} \mathop{\mathrm{avg}}\limits_{p\in\bm{x}\,\mathrm{in}\,r_i}L_2\left(q_i, p\right)\right)
\end{equation}
where $p\in\bm{x}\ \mathrm{in}\ r_i$ represents all pixels of image $\bm{x}$ in specified region $r_i$. Due to performance reasons, each image in the database is represented only as $20\times 15$ color points or `pixels'. Our user interface also currently supports regions in the shape of ellipses only. If a keyword or color-sketch search is not successful or a user wants to simply browse similar images, our tool utilizes deep features from GoogLeNet~\cite{szegedy2015going} after the last average pooling layer for retrieval of semantically similar images. Rank of an image given a query example is the cosine similarity between their deep feature vectors.


All the retrieval models provide a relevance score function inducing a similarity based ordering of all database objects with respect to a given query. Even though our tool supports multiple query modalities at once by normalizing the ranking of each modality to the interval $[0, 1]$ and then summing them up, combining multiple models can be tricky. This is because each model has a different distribution of rank values, e.g. similarity ranking assigns to most of the database rank between $0.6$ and $0.85$ whereas in keyword ranking almost all images are assigned rank smaller than~$10^{-2}$. However, our tool enables us to use other models as filters where their thresholds can be dynamically set for each model interdependently. Such an approach proved to be much more useful than sorting joined ranking from multiple models.

