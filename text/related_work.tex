\chapter{Related Work}
In this section, we review recent approaches towards textual annotation. First we discuss available datasets, then multiple approaches in image classification. Lastly we examine object localization in an image. Unfortunately we did not include video annotation in out work since video processing and training creates unfeasible demands on computational power.

% and finally we look on image sequence annotation. Last part will be just a list of possible methods since all of them require unfeasible demands on computational power.

\section*{Datasets}
\addcontentsline{toc}{section}{Datasets}
Recent breakthroughs in machine learning could not happened without large-scale datasets since they are necessary to train and evaluate algorithms. Image datasets can be divided into three groups depending on what they are addressing:~image classification, object localization and semantic segmentation.

Objective of image classification tasks is to decide whether given object is or is not present on an image. There are multiple low-resolution datasets such as CIFAR-100~\cite{krizhevsky2009learning} containing 60,000 images in 100 categories with resolution 32-by-32 pixels. Since 2009 there is ImageNet~\cite{ILSVRC15}, a large-scale high resolution dataset with aims to populate the majority of the 80,000 WordNet synsets with 500 to 1000 images. In research a subset of the ImageNet database called ILSVRC2012 containing 1000 categories is usually used.

Task of object localization embodies stating what is in an image and where in the image it is. The location is usually given as an bounding box. As of 2018, ImageNet database holds bounding boxes for over 3000 synsets with an average of 150 images per synset. Similarly to ImageNet, Open Images Dataset~\cite{openimages} contains over one and half million annotated images with more than twice as many bounding boxes belonging into 600 categories. Semantic segmentation goes a step further. Its goal is to not only distinguish and localize object in an image but to assign each pixel to an object it is belonging to. Probably the most recognized dataset The Microsoft Common Objects in COntext (MS COCO)~\cite{lin2014microsoft} contains 91 common object categories with 82 of them having more than 5,000 labeled instances. There are also many more datasets focused on specific task such as Places~\cite{zhou2017places}, designed for scene recognition, or YFCC100M~\cite{YFCC100M} used for unsupervised learning.

\section*{Image Classification}
\addcontentsline{toc}{section}{Image Classification}
Prior to 2012 state-of-the-art approaches towards image classification involved using SVM classifiers trained on handcrafted features. Even though those methods can be tweaked, they rise and fall with a quality of the features. Since 2010 ImageNet Large Scale Visual Recognition Challenge (ILSVRC)~\cite{ILSVRC15} has been used to benchmark computer vision systems. Their dataset contains 1000 categories, each with around 1000 high resolution images. Top-5 error rate for the SVM classifiers hovered at over 25\% until in 2012 an entry from A.~Krizhevsky,~et~al.~\cite{AlexNet} disturbed machine learning community with by far the best result of 15.3\% using deep convolution neural network (DCNN) and pushed the whole industry towards neural networks, which until then were used only in limited number of cases such as handwritten character recognition. The main advantage of neural networks is that they, in contrast to a SVM classifiers, can learn the best features themselves and thus they seem ideal for such tasks. But we were unable to train larger models due to multiple issues that were overcome in the last few years.

Fast graphic cards, novel stochastic optimization algorithms \cite{kingma2014adam}, clever weight initialization \cite{glorot2010understanding}, non-saturating activation functions, dropout \cite{srivastava2014dropout} and many other techniques allowed for deeper and bigger networks. One of the key ideas, so called network in network, takes elementary building blocks such as convolutional and max pooling operations with different filter sizes and stacks them into larger, more complex layers, sometimes called blocks or cells. This idea has been used in efficient deep neural network architecture codenamed Inception~\cite{szegedy2015going} which uses 1x1, 3x3 and 5x5 convolution layers together with max pooling side-by-side (Figure \ref{fig:inception_block}). Additional pointwise (1x1) convolution (in gray) is used to project the output of a previous layer onto a new filter space, greatly reducing number of trainable parameters in 3x3 and 5x5 convolutions. This enabled GoogLeNet~\cite{szegedy2015going}, comprised of these blocks, to win with 6.7 top-5 error rate ILSVRC competition in 2014, defeating 20 times larger DCNN by Simonyan and Zisserman~\cite{simonyan2014very} only made of classical convolutional layers.

\begin{figure}
	\centering
	
	\begin{tabular}{@{}c@{}}
		\subfloat{
			\input{img/inception_block.tikz}
		}
	\end{tabular}
	\qquad\qquad
	\begin{tabular}{@{}c@{}}
		\subfloat{
			\input{img/residual_connection.tikz}
		}
	\end{tabular}
	
	\caption[Various advanced DCNN architectures]{Inception block of GoogLeNet~\cite{szegedy2015going} (2014, left) and skip connection of a residual network~\cite{he2016deep} (2015, right).}
	\label{fig:inception_block}
\end{figure}

By looking at those described networks it seems that adding more layers is beneficial. But increasing depth of suitably deep models can lead to accuracy degradation~\cite{he2016deep}. Yet this is not caused by overfitting since we can also observe higher training error. It could be partly due to vanishing or exploding gradients even though there are methods to moderate the problem such as clever weight initialization or gradient normalization. To tackle the problem a deep residual learning framework~\cite{he2016deep} was proposed. Instead of training a set of layers to fit a desired underlying mapping $\mathcal{H}(\bm{x})$ directly, it lets layers to fit a residual mapping $\mathcal{F}(\bm{x})-\bm{x}$ such that $\mathcal{H}(\bm{x}) = \mathcal{F}(\bm{x})+\bm{x}$. In neural networks it can be realized by ``skip connections'' (Figure~\ref{fig:inception_block}). ResNet-152, a DCNN utilizing this idea  with whooping 152 layers, achieved 4.49\% top-5 error rate on ILSVRC in 2015.

The most recently, attempts to automate network's architecture engineering process were made using reinforcement learning (RL)~\cite{zoph2016neural,zoph2017learning}. The methods utilize a controller recurrent neural network (RNN) that predicts a network architecture. This architecture is then trained and its achieved accuracy is used to scale the gradients to update the RNN controller. Despite being extremely computationally demanding needing thousands of GPU-hours even on small datasets, \cite{zoph2017learning} shows that architectures learned on CIFAR-10 can translate easily to ImageNet's ILSVRC achieving best results ever with 3.8\% top-5 error rate. Even smaller networks using the same structure outperform equivalently-sized human-designed models. A cell of the best performing architecture can be seen in Figure~\ref{fig:nasnet_block}.

\begin{figure}
	\centering
	\input{img/nasnet_block.tikz}
	
	\caption[A state-of-the-art block of NASNet-A]{A state-of-the-art block of NASNet-A~\cite{zoph2017learning} (2017), created by a recurrent neural network, outperformed all blocks ever designed by humans.}
	\label{fig:nasnet_block}
\end{figure}

\section*{Other Approaches to Annotation}
\addcontentsline{toc}{section}{Other Approaches to Annotation}

It is not uncommon an image given for annotation contains dozens of objects, then the classical approach to image classification starts to fall apart since a standard softmax classification layer prefers to assign only one class. Ordinarily multiple cutouts of an image are taken and annotated separately, the final classification is a mean of distributions for each cutout. It may be sufficient for the task but it does not give us locations of the objects. To obtain a bounding box of an object in an image, a region-based convolutional network (R-CNN) has been proposed~\cite{ren2015faster}. It utilizes two deep convolutional networks, a region proposal network (RPN) suggesting regions of interest and a standard classification DCNN annotating suggested regions. RPN uses first convolutional layers of a standard DCNN such as \cite{simonyan2014very} to generate a feature map. Then a small network takes a $n\times n$ spatial window of the feature map and predicts $k$ bounding boxes, for each box also stating probability the it contains an object.