\chapter{Model Evaluation on KIS Task}
In Chapter \ref{} we evaluated the our model on classification task which differs from KIS task. In this chapter we therefore discuss differences between classification and know-item search, propose possible evaluation methods and present their results. We also show and discuss this year's Video Browser Showdown competition we successfully participated in.

In KIS task the goal of a retrieval model parametrized by $\bm{\varOmega}$ is to minimize a position (or rank) $r_{(\bm{x},Q)}^{\bm{\varOmega}}$ of the searched image $\bm{x}$ given a user query $Q$. If the position is treated as a random variable the objective can be rephrased in terms of probability theory as
\begin{equation}
\mathop{\arg\max}_{\bm{\varOmega}}\mathbb{E}_{\left(\bm{x},Q\right)\sim \hat{p}_{data}}\left[
\int_{t=1}^{|C|}p_{model}\left(r_{(\bm{x},Q)}^{\bm{\varOmega}}\leq t\right)
\right]\label{eq:kis}
\end{equation}
where $p_{model}\left(r_{(\bm{x},Q)}^{\bm{\varOmega}}\leq t\right)$ is probability image's position is less than or equal to~$t$ and $|C|$ is size of the collection. Intuitively, the ideal user constructing the most specific queries~$Q$ for images~$\bm{x}$ together paired with ideal model capable of distinguishing all images from each other would have always ${p_{model}\left(r\leq 1\right)=1}$, therefore yielding the highest possible value. Unfortunately, unlike the function \ref{eq:mle} of a classification task, function \ref{eq:kis} is not differentiable thus we can only use it as quality measure of our model. Yet another issue arises with $\hat{p}_{data}$ distribution since it is dependent on set of supported labels and on a dataset the images $\bm{x}$ are taken from. In order to evaluate our model thorough user study would be needed yet a slight change to the model would nullify its results. In the next section we construct simple simulation providing us with an insight into model's performance without the need for a broad study.

\section{User Simulations}
To simplify the simulations only queries containing \textsf{OR} will be considered. We can therefore write $Q=\{N_1\}$ and for a sake of simplicity we will use $Q=N_1\subseteq L_m$.
We take inspiration from~\cite{kovalvcik2017comparison} on approaches toward generating user queries and define following users:
\begin{description}[labelwidth=1em, leftmargin=!]
	\item \textbf{Ideal User.} Assumes a user is coherent with the model $\bm{\theta}$. Query $Q_i$ of length~$|Q_i|$ for a random image $\bm{x}$ is generated by drawing $|Q_i|$-times from set $\{1,\dots,L_m\}$ with probability $p(label)=\bm{\hat{y}}_{label}$ where $\bm{\hat{y}}=f\left(\bm{x}, \bm{\theta}\right)$ is the distribution of classes predicted by our neural network.
	\item \textbf{Real User.} A human judge is given a random image $\bm{x}$ from a dataset and describes it in set of labels $Q_r\subseteq L_m$ generating one $(\bm{x},Q_r)$ pair.
	\item \textbf{Modeled User.} Given a set of image-query pairs generated by humans we can infer distribution $\mathcal{C}$ how likely user selects top-$k^{\mathrm{th}}$ label the neural network assigned to an image for all $k\in\{1,\dots,|L_m|\}$.
	Query $Q_m$ of length~$|Q_m|$ for a random image $\bm{x}$ is generated by drawing $|Q_m|$-times $c$ from distribution $\mathcal{C}$ and taking top-$c^{\mathrm{th}}$ label from $\bm{\hat{y}}$.
\end{description}
VBS competition uses TRECVid Internet Archive Creative Commons (IACC.3) dataset~\cite{awad2016trecvid} containing 600 hours of recordings in 4593 videos making it a good candidate dataset for the simulations. Since our model is based on images, we only considered 335,944 master shot reference (MSR) keyframes~\cite{MasterShotReference}\footnote{During the actual competition, our tool worked with bigger set of keyframes to better deal with competition's challenges.}.

To create modeled user, the author annotated 250 random images from MSR collection by one to five labels from set $L$ (transformed to subset of $L_m$ by rules described in chapter \ref{sec:query_formulation_and_ranking}). During the annotation, images containing random noise and ornaments as well as some low quality shots were skipped. Further, the distribution $\mathcal{C}$ how likely user selects top-$k^{\mathrm{th}}$ label was created (Figure \ref{fig:class_distribution}). We tried to smooth the distribution but it usually deteriorated the results therefore we kept using the unmodified one. Comparison of ideal, real and modeled user is shown in Figure \ref{fig:simulation_keyword}. We can see not surprisingly that ideal user is optimistic estimate of the real user whereas modeled user is pessimistic estimate. The difference between real and modeled user is profound especially for rank in first dozens of thousands. We suspect this is because if real user selects top-$1^{\mathrm{st}}$ label, such a label is quite unique and distinguishable in the collection. On the other hand modeled user can select top-$1^{\mathrm{st}}$ label on random noise image where its rank can be unpredictable.

\begin{figure}
	\centering
	
	\begin{tabular}{@{}c@{}}
		\subfloat{
			\input{img/simulation_keyword_ideal.tikz}
		}
	\end{tabular}
	\begin{tabular}{@{}c@{}}
		\subfloat{
			\input{img/simulation_keyword_modeled.tikz}
		}
	\end{tabular}
	\input{img/simulation_keyword_thresholds.tikz}
	
	\caption[Comparison between ideal, real and modeled user]{Comparison between ideal ($Q_i$) and real ($Q_r$) user on the left and between modeled ($Q_m$) and real on the right. Horizontal axis shows position $t$ and vertical axis shows probability $p(r_{\bm{x}}\leq t)$ how likely random image will be in first $t$ images given query constructed by ideal, real or modeled user. Bottom graph shows how discarding all labels with probability less than $\mu$ affects quality of ranking. Based on 2000 queries of ideal and modeled user and 250 queries of real user.}
	\label{fig:simulation_keyword}
\end{figure}
Simulations clearly show that keyword retrieval model can alone, even in the most optimistic case, find only 50\% of queries considering only ranks up to small thousands since browsing in even 10 thousand images cannot be done in reasonable time. But on the other hand the graphs show that approximately 10\% of searched images can be found in first few hundreds indicating that some browsing can be beneficial. We can also see that extending the query by new labels increases chance of success even in case of the conservative modeled user.
To reduce memory requirements of our retrieval model we also introduce a threshold $\mu$ discarding all labels with probability smaller than the threshold. Using simulations we investigate how different values of $\mu$ affect quality of the ranking. As can be seen in Figure \ref{fig:simulation_keyword} (bottom) using $\mu=0.005$ deteriorates quality of ranking quite significantly whereas threshold of $0.001$ has negligible effect. Therefore in production $\mu = 0.001$ is used yielding to approximately 80 labels per image on average, reducing the inverted index size to only 7 percent of the original size.

\begin{figure}
	\centering
	\input{img/simulation_keyword_histogram.tikz}
	
	\caption[Agreement between user and neural network labeling]{Distribution $\mathcal{C}$ how likely user selects top-$k^{\mathrm{th}}$ label the neural network assigned to an image. Ideally if user was fully coherent with the neural network $p(1)=1$ and $p(k)=0$ for all other $k\in\{2,\dots,1150\}$.
	}
	\label{fig:class_distribution}
\end{figure}


\subsection{Similarity Re-Ranking}
Many interactive video retrieval tools enable user to query a database in many modalities. The most relevant shots (images) $D$ are then displayed to the user. If searched image $\bm{x}$ is not present in set $D$, usually user can sequentially browse the results. However, as datasets get bigger this approach quickly becomes unfeasible. Retrieval systems therefore provide an option of similarity browsing based on handcrafted features or more recently deep neural network features~\cite{lokovc2018revisiting, barthel2018fusing} speeding up the search if visually and/or semantically similar image is present in set $D$. Therefore we evaluate our model together with similarity browsing based on our networks deep features to better estimate model's performance in known-item search task. We take the definition of similarity user from \cite{kovalvcik2017comparison}:
\begin{description}[labelwidth=1em, leftmargin=!]
	\item Ideal \textbf{Similarity User.} Given an target image $\bm{x}$ and set of most relevant images $D$, the most similar image to $\bm{x}$ from $D$ is selected.
\end{description}
We define similarity of two images as cosine of the angle between their activation values in the last average pooling layer of GoogLeNet $f'(\cdot;\bm{\theta})$:
\begin{equation}
similarity(\bm{a},\bm{b}) = \frac{\langle f'(\bm{a};\bm{\theta}),f'(\bm{b};\bm{\theta})\rangle}{\|f'(\bm{a};\bm{\theta})\|\|f'(\bm{b};\bm{\theta})\|}
\end{equation}
where $\bm{a}$ and $\bm{b}$ are two images, $\langle\cdot,\cdot\rangle$ is dot product and $\|\cdot\|$ is $L_2$ norm.

In the simulations ideal, real and modeled keyword users are used to assign each image in a collection initial rank and $|D|$ top ranking images are passed to similarity user. Even though, as reported by \cite{kovalvcik2017comparison}, humans usually do not select the most similar image based on our $similarity$ measure we can use this optimistic approximation to show theoretical limits of our model and to compare how well keyword initialization helps with \textit{page zero problem} of similarity browsing.
Results in Figure \ref{fig:simulation_rerank} shows that one re-ranking significantly improves chance of success whereas added value of every other re-ranking lowers probably because we converge to some cluster in high dimensional feature space. We can also see that even given ideal keyword user and ideal similarity user 20\% of images we not found therefore proving interactive search using other approaches such as color or edge sketches as necessity. With respect to the results of simulations with random query initialization we argue that such small gap between random and modeled user after three re-rankings is due to target dataset's uniformity where keyword initialization cannot help much. Searching for dataset's distinct objects using only similarity browsing is on the other hand based on our experience with the dataset almost impossible.

\begin{figure}
	\centering
	\begin{tabular}{@{}c@{}}
		\subfloat{
			\input{img/simulation_rerank_0.tikz}
		}
	\end{tabular}
	\begin{tabular}{@{}c@{}}
		\subfloat{
			\input{img/simulation_rerank_1.tikz}
		}
	\end{tabular}
	\begin{tabular}{@{}c@{}}
		\subfloat{
			\input{img/simulation_rerank_2.tikz}
		}
	\end{tabular}
	\begin{tabular}{@{}c@{}}
		\subfloat{
			\input{img/simulation_rerank_3.tikz}
		}
	\end{tabular}
\begin{tikzpicture}
	\node [draw=none,font=\footnotesize, fill=none,anchor=north west] at (0,0) {Keyword user: \ref{plot:purple} Real\quad\ref{plot:solid} Ideal\quad\ref{plot:dashed} Modeled\quad\ref{plot:dotted} Random};
\end{tikzpicture}


	\caption[Influence of similarity browsing on ranking]{Influence of similarity browsing on ranking. Horizontal axis shows position $t$ and vertical axis shows probability $p(r_{\bm{x}}\leq t)$ how likely random image will be in first $t$ images given query constructed by ideal, real, modeled or random user and number of re-rankings by similarity user with $D$ being $50$ highest ranked images from the previous iteration or keyword initialization. Based on 1000 queries of ideal, modeled and random user and 250 queries of real user.}
	\label{fig:simulation_rerank}
\end{figure}

\section{VBS Competition}

Since 2012, The Video Browser Showdown competition~\cite{cobarzan2017interactive,Lokoc-influential-trends}, organized at the \textit{International Conference on MultiMedia Modeling}, serves as a stage where researches evaluate their interactive video retrieval tools. SIRET group of Charles University debuted at the competition in 2014~\cite{Lokoc-VBS2014}, winning it in years 2014, 2015 and newly with our keyword-based retrieval model in 2018~\cite{lokovc2018revisiting}. In this section, we can therefore introduce the competition and discuss our tool's results.

VBS competition focuses on known-item search (KIS) and ad-hoc video search (AVS) tasks in highly interactive
way with human in the loop rather than using only automated retrieval systems. Such task can highly differ from just automated systems and maybe surprisingly some tools~\cite{Storyboard-Based_Interface,duane2018virtual} with little to no automated retrieval models can hold up to tools designed on deep learning models. In 2018, VBS comprised of three types of tasks on TRECVid's IACC.3 600 hour dataset~\cite{awad2016trecvid}: \textit{(1)} Visual known-item search (KIS) task with users searching short selected clip played in loop but with clip getting continually more and more blurred to prevent teams to focus on one image of the clip only. Also recording the played clip is forbidden since such a task is addressed in other computer vision benchmarks. \textit{(2)} Textual known-item search (KIS) task with users searching for short clip only described by text. Due to its inherent difficulty, more information appears over time. \textit{(3)} Ad-hoc search (AVS) task where users are given short text description and goals is to find as many scenes as possible corresponding to the description.

\subsection{Scoring}
For visual and textual KIS tasks following formula was used at VBS 2018:
\begin{equation}
\max\left(0, 50 + 50 (t_L-t) - 10\abs{WS}\right)
\end{equation}
where $t_L$ is duration of the task (5 minutes for visual and 7 minutes for textual~KIS), $t$ is time of submission and $\abs{WS}$ is number of wrong submissions before the correct one. For AVS task time independent function was used:
\begin{equation}
\frac{|C|}{|C|+|I|/2}\cdot\frac{|q(C)|}{|q(P)|}
\end{equation}
where $|C|$ is number of correct and $|I|$ number of incorrect team's submissions and $P$ is set of all correct submissions by all teams representing artificial recall of 100\%. Function $q(\cdot)$ clusters frames of one scene with the same action into one frame encouraging teams to find different videos (or segments) rather than to send all frames of one scene (see~\cite{Lokoc-influential-trends} for detailed explanation).

During the competition there were five runs each with four tasks -- visual KIS, textual KIS and AVS rounds with tools being operated by experts (usually creators of the tools) and another visual KIS and AVS rounds with tools operated by novice users selected from conference visitors. The overall score was determined by averaging all five rounds with a winner of each round getting 100 points and others proportionally to the winner.

\begin{table}
	
	\centering
	\sisetup{detect-weight=true,detect-inline-weight=math}
	\begin{tabular}{l@{\hspace{0.5cm}}S[table-format=3.0]S[table-format=3.0]S[table-format=3.0]S[table-format=3.0]S[table-format=3.0]S[table-format=3.0]}
		\toprule
		\multirow{2}{*}{\textbf{Team}} & \multicolumn{1}{c}{Visual} & \multicolumn{1}{c}{Textual} & \multicolumn{1}{c}{\multirow{2}{*}{AVS}} & \multicolumn{1}{c}{Visual KIS} & \multicolumn{1}{c}{AVS} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Overall}}} \\
		& \multicolumn{1}{c}{KIS} & \multicolumn{1}{c}{KIS} & & \multicolumn{1}{c}{Novice} & \multicolumn{1}{c}{Novice} & \\
		\midrule
		SIRET (our)~\cite{lokovc2018revisiting}& 95\por{2}&\bftabnum 100\por{1}&    71\por{4}&           67\por{4}&    83\por{2}& \bftabnum 83\\
		ITEC1~\cite{ITEC1}      & 64\por{6}&           37\por{5}&    60\por{7}&\bftabnum 100\por{1}&    64\por{4}& 65\\
		ITEC2~\cite{ITEC2}      & 29\por{8}&           47\por{3}&    81\por{2}&           85\por{3}&    75\por{3}& 63\\
		HTW~\cite{barthel2018fusing}        & 91\por{3}&           41\por{4}&    61\por{6}&           50\por{5}&    61\por{5}& 61\\
		NECTEC~\cite{NECTEC}     & 68\por{5}&            0&\bftabnum 100\por{1}&            0&\bftabnum 100\por{1}& 54\\
		VIREO~\cite{VIREO}      &\bftabnum 100\por{1}&  0&           50\por{8}&           86\por{2}&    30\por{8}& 53\\
		VITRIVR~\cite{VITRIVR}    & 79\por{4}&           55\por{2}&    64\por{5}&            0&           35\por{7}& 47\\
		VERGE~\cite{VERGE}      & 62\por{7}&            0&           72\por{3}&            0&           46\por{6}& 36\\
		VNU~\cite{VNU}        & 21\por{9}&            0&           41\por{9}&           48\por{6}&    14\por{9}& 25\\
		\bottomrule
		\multicolumn{7}{l}{\footnotesize ($\cdot$) Standing in individual round.}
	\end{tabular}
	
	\caption[VBS 2018 results]{VBS 2018 results.}
	\label{fig:vbs_results}
\end{table}

\subsection{Results}
The results of VBS 2018 are presented in Table \ref{fig:vbs_results} where it can be seen we won by significant margin scoring 27 percent more points than the second team. However in Table \ref{fig:vbs_tasks_timeline} we can see there are still areas for improvement especially in the textual KIS tasks where two of four tasks were not solved by any team. But we can proudly report that SIRET was the only team to successfully solve both solved textual KIS tasks and all expert visual KIS task yet we finished the round second after a team with only three solved tasks because of the wrong submission penalty in the first task. Interestingly even though strong in the expert visual run, in the novice run we were not able to solve two out of four tasks. This trend of lower performance in novice run can be however observed with all teams except ITEC1 and ITEC2 with the former being the only team solving three out of four tasks.

\begin{table}[h]
	
	\centering
	\sisetup{detect-weight=true,detect-inline-weight=math}
	\begin{tabular}{cccc|cccc|cccc|cccc|cccc}
		\toprule
		$\substack{V \\ E}$ & $\substack{V \\ E}$ & $\substack{V \\ E}$ & $\substack{V \\ E}$ & $\substack{T \\ E}$ & $\substack{T \\ E}$ & $\substack{T \\ E}$ & $\substack{T \\ E}$ & $\substack{A \\ E}$ & $\substack{A \\ E}$ & $\substack{A \\ E}$ & $\substack{A \\ E}$ & $\substack{V \\ N}$ & $\substack{V \\ N}$ & $\substack{V \\ N}$ & $\substack{V \\ N}$ & $\substack{A \\ N}$ & $\substack{A \\ N}$ & $\substack{A \\ N}$ & $\substack{A \\ N}$ \\
		\midrule
		2 & 4 & 8 & 7 & \textcolor{red}{0} & 3 & \textcolor{red}{0} & 3 & 9 & 9 & 9 & 9 & \textcolor{red}{1} & 3 & 4 & \textcolor{red}{3} & 9 & 9 & 9 & 9 \\
		\bottomrule
	\end{tabular}
	
	\caption[Number of successful teams per task at VBS 2018]{Number of successful teams per video KIS (V), textual KIS (T) and AVS~(A) task with tool being operated by experts (E) or novices (N). Red tasks were not solved by our tool.}
	\label{fig:vbs_tasks}
\end{table}


Since VBS 2018 mandatory logging of user actions was established such that with each frame sent to the server basic set of actions that lead to the result was submitted. Even though complete analysis will be published in separate journal, we can at least show and analyze logs from our tool presented in Figure~\ref{fig:vbs_tasks_timeline}. Looking at the visual KIS expert tasks we can see that both keyword and color sketch can be equally good initialization of a display from which we can semantically browse the collection. On the other hand in the textual KIS tasks, keyword plays the utmost role in success with all teams starting with keyword queries except ITEC2 which utilized their advanced color sketch interface but in the end falling back on keyword search anyway. When comparing expert and novice users we can see expert's more systematic approach utilizing less models at once. At least in the second and fourth expert visual task we can with high probability say user has been systematically using color sketch as a filter wheres novices with their rapid model changes could hardly do some systematic filtering. In the AVS tasks we lost some potential since our tool was not able to quickly send whole range of images yet our .... proved to be useful especially for novices. Visual representation of AVS logs can be seen in the Attachment \ref{}.

\begin{figure}
	\centering
	\input{img/vbs.tikz}
	
	\caption[Use of tool's retrieval models in KIS tasks]{Use of our tool's retrieval models in all solved visual and textual KIS task at VBS 2018. Horizontal axis represents time since task's start, green and red arrows represent correct and incorrect submissions to the competition's server.}
	\label{fig:vbs_tasks_timeline}
\end{figure}
