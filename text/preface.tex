\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}


With the advent of affordable portable electronics, especially mobile phones, world's digital space has been flooded by user generated content, specifically photos and videos. Together with cheaper storage options, we are facing so called explosion of multimedia. Bigger than ever collections pose significant challenges for users to find what they are searching for. Challenges arise not only in personal videos. Miniaturization enabled new types of collections such as medical surgery or police body camera recordings.

The growing amount of data requires new effective approaches to image and video retrieval, because traditional methods were not able to deal efficiently with such a high dimensionality of an image or a video. There are two backbones of recent progress. The first are cheaper, more powerful GPUs. The second is creation of large-scale datasets, such as ImageNet~\cite{ILSVRC15} and TRECVid~\cite{2017trecvidawad}, that enabled us to experiment with more sophisticated, but also more computationally demanding, models. All that led to recent breakthroughs in machine learning, especially artificial neural networks. It gave birth to new applications in fields of computer vision, speed recognition, text translation and many others.

Nowadays in computer vision, commercial services such as Google Images need not to rely on human labeling either on web page or by an external worker. Latest neural networks can, on some datasets, even exceed human performance~\cite{he2015delving}, thus one could assume it is a solved problem. Yet there are not many, if any, not task specific tools the author is aware of, that provide user an interface to organize and search in their collections for whatever their need is. One could argue Google Photos and YouTube have all the user's content three clicks away, but try to find a tank in thousands of images or videos respectively from dashboard cameras in either one of those services.

The described scenario is called known-item search (KIS), where a user searches for an exact scene in a database. The KIS tasks can be further divided based on a knowledge user has about the given scene. The first, and usually easier to solve, KIS task is visual. This means a user has already seen the searched scene, but, for example, needs to find it in a collection as a proof. The other type of task is textual, the harder, but maybe more important, counterpart to the video task. Useful when police tries to find a subject on CCTV cameras or rescue workers a missing plane on satellite imagery. In the visual task, a user can specify a similar object for example from the Internet, where images are usually textually labeled, thus reducing the problem to similarity search. This new problem, however difficult it may seem, can be solved by employing deep neural networks trained on some non related image task~\cite{deepFeatures}. These networks learn highly non-linear transformations from image space to arbitrary feature space in which common metrics can be used to find semantically similar images.
To eliminate the need for an example object, that can be hard to come by, many low level features were proposed, such as motion~\cite{motionSketch}, edge~\cite{ITEC2} or color~\cite{sigBrowser} sketches. The last approach winning Video Browser Showdown (VBS)~\cite{cobarzan2017interactive} in 2014 and 2015. A competition where state-of-the-art interactive video retrieval tools demonstrate their efficiency on an in advance known dataset (in 2018 the TRECVid IACC.3 with 600 hours of video content).

The textual task presents much bigger challenges since, in most cases, visual features cannot be used. Especially as datasets grow exponentially, handcrafted low level features are no longer viable option since a researcher cannot manually look through the whole dataset to design and tune retrieval engine for the dataset's specifics. One way to solve lack of searchable features is to improve browsing. Some work has been done on on-line approximation of relevances of individual images~\cite{suditu2011heat} when user has already seen a part of a database. Others try to overcome drawbacks of two dimensional grid interface presenting image thumbnails by proposing to use 3D interfaces~\cite{schoeffmann20143}, but none of this aims to solve the lack of usable features. Fortunately the advent of neural networks and deep learning brought new methods for dealing with automated multimedia annotation. Nowadays there are architectures capable of not only assigning images to classes but also describing them in sentences~\cite{vinyals2017show}. Other neural networks can extract useful information from multiple subsequent frames of videos~\cite{bertinetto2016fully}, such as tracking moving object in time. But when used on KIS tasks, those systems produce little to no usable results, since some teams at VBS were unable to successfully complete any textual tasks~\cite{Lokoc-influential-trends} and others did not use textual annotation at all.

We try to examine this usability gap and summarize possible approaches towards textual annotation. Then we select viable model and propose modifications to mitigate its shortcomings. Later on, we try to estimate performance of the model by simulating user, a task needed to select best strategy and set correct expectations when actually using the model in video retrieval tool. Finally, we present award winning tool\footnote{The tool competed at Video Browser Showdown~\cite{cobarzan2017interactive} and outperformed other participants' tools by more than 27 percent. In textual tasks, the tool performed even better.} for know-item search. Since the tool is jointly developed with Jakub Lokoč and Gregor Kovalčík, the version on the attached CD will only demonstrate main ideas of this thesis that are solely author's work. Some of the ideas were also published in \cite{lokovc2018revisiting, LokocLSC}.

