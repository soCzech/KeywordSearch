\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}


With the advent of affordable portable electronics, especially mobile phones, the world's digital space has been flooded by user-generated content, specifically photos and videos. Together with cheaper storage options, we are facing a so-called explosion of multimedia. Bigger than ever collections pose significant challenges for users to find what they are searching for. Challenges arise not only in personal videos. Miniaturization enabled new types of collections such as medical surgery or police body camera recordings.

The growing amount of data requires new effective and efficient approaches to image and video retrieval. There are two backbones of recent progress. The first is cheaper and more powerful GPUs. The second is a creation of large-scale datasets, such as ImageNet~\cite{ILSVRC15} and TRECVid~\cite{2017trecvidawad}, that enabled researchers to experiment with more sophisticated, but also more computationally demanding, models. All that led to recent breakthroughs in machine learning, especially artificial neural networks. It gave birth to new applications in fields of computer vision, speech recognition, text translation and many others.

Nowadays in computer vision, commercial services such as Google Images need not rely on human labeling either on a web page or by an external worker. Latest neural networks can, on some datasets, even exceed human performance~\cite{he2015delving} thus one could assume it is a solved problem for specific domains. Yet there are not many, if any, not task-specific tools the author is aware of, that provide a user an interface to organize and search in their collections for whatever their need is. One could argue that Google Photos and YouTube have all the user's content three clicks away, but their focus is different from helping a user to find a tank in thousands of images or videos respectively from dashboard cameras which is difficult in either one of those services.

Such a scenario where a user searches for an exact known scene in a database is called known-item search (KIS). KIS tasks can be further divided based on a knowledge user has about the searched scene. The first, and usually easier to solve, KIS task is visual. This means a user has already seen the searched scene before, but, for example, needs to find it in the collection as a proof. The other type of KIS task is textual, the harder, but maybe more important, counterpart to the visual task.
A possible solution to KIS task is to specify a similar object for example from the Internet, where images are usually textually labeled, and use the object's visual and semantic features as a query in the task's collection. A popular approach for extracting such features is by employing deep neural networks trained on some non-related image task~\cite{deepFeatures}. These networks learn highly non-linear transformations from image space to arbitrary feature space in which common metrics can be used to find semantically similar images.
To eliminate the need for an example object, that can be hard to come by, many low-level features were proposed, such as motion~\cite{motionSketch}, edge~\cite{ITEC2} or color~\cite{sigBrowser} sketches. The third approach winning the Video Browser Showdown (VBS)~\cite{cobarzan2017interactive} in 2014 and 2015. VBS is a competition where state-of-the-art interactive video retrieval tools demonstrate their performance on an in advance known dataset (in 2018 the TRECVid IACC.3 with 600 hours of video content).

The textual task presents much bigger challenges since, in most cases, visual features are not available to initiate the search. One way to solve the lack of features is to improve browsing. Some work has been done on an online approximation of relevance of individual images~\cite{suditu2011heat} when a user has already seen a part of a database. Others try to overcome the drawbacks of two-dimensional grid interface presenting image thumbnails by proposing to use 3D interfaces~\cite{schoeffmann20143}, but none of this aims to solve the absence of usable features for query initialization. Fortunately, the advent of neural networks and deep learning brought new methods for dealing with automated multimedia annotation. Nowadays there are architectures capable of not only assigning images to classes but also describing them in sentences~\cite{vinyals2017show}. Other neural networks can extract useful information from multiple subsequent frames of videos~\cite{bertinetto2016fully}, such as tracking moving object in time. But when used for KIS tasks on the actual VBS dataset, those systems produce little to no usable results, since some teams at VBS were unable to successfully complete any textual tasks~\cite{Lokoc-influential-trends}.

We try to examine this gap and summarize possible approaches towards textual annotation. Then we select a viable model and propose modifications to mitigate its shortcomings. Later on, we try to estimate the performance of the model by simulating user, a task to identify promising search strategies and investigate expectations when actually using the model in a video retrieval tool. We also present ideas of an award-winning tool\footnote{The tool competed at Video Browser Showdown~\cite{cobarzan2017interactive} and outperformed other participating tools in the overall score. In textual tasks, the tool performed even better.} for known-item search jointly developed with Dr. Jakub Lokoč and Bc. Gregor Kovalčík. For that reason, the application prototype on the attached CD will only demonstrate the main ideas of this thesis that are solely the author's work. Some of the ideas were also published at international conferences in the following papers:
\begin{enumerate}
	\item \textbf{Revisiting SIRET Video Retrieval Tool}~\cite{lokovc2018revisiting}\\
	Demo paper describing our retrieval tool.
	\item \textbf{Using an Interactive Video Retrieval Tool for LifeLog Data}~\cite{LokocLSC}\\
	Demo paper introducing new ideas added to the tool to better deal with monotonous lifelog domain.
\end{enumerate}
with the first being accepted to Video Browser Showdown at International Conference on MultiMedia Modeling 2018 and the second accepted to Lifelog Search Challenge workshop at International Conference on Multimedia Retrieval 2018.
